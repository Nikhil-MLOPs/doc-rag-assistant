ğŸ“„ Doc-RAG-Assistant

AI-Powered Document Q&A System | FastAPI ğŸš€ LangChain ğŸ¤– ChromaDB ğŸ“š MLflow ğŸ“Š Docker ğŸ³ AWS EC2 â˜ï¸

ğŸŒŸ Project Overview

Doc-RAG-Assistant is a production-ready Document Retrieval-Augmented Generation (RAG) system.

It allows users to upload PDFs or text files, extracts content, splits it into chunks, stores it in a vector database, and enables users to ask questions with accurate, context-based answers.

Built using FastAPI, LangChain, HuggingFace Embeddings, ChromaDB, Ollama LLM, MLflow, Docker, and deployed on AWS EC2.

ğŸš€ Key Features

âœ” Upload files (PDF, TXT) via Web UI

âœ” RAG-based intelligent question answering

âœ” Embedding and vector search using ChromaDB

âœ” Uses Mistral LLM (via Ollama) for context-based answers

âœ” MLflow-based logging (latency, answer stats, sources)

âœ” Dockerized and deployed on AWS EC2

âœ” CI/CD ready with GitHub Actions

âœ” Future support for Kubernetes, Prometheus & Grafana

ğŸ§  System Architecture

Frontend (HTML/JS) â†’ FastAPI Backend â†’ Document Processing â†’ ChromaDB â†’ Ollama LLM â†’ MLflow Logging

ğŸ§  Technologies Used

Layer	Technology

Backend	FastAPI

RAG Engine	LangChain

Embeddings	HuggingFace

Vector DB	ChromaDB

LLM	Ollama (Mistral)

Monitoring	MLflow

Frontend	HTML, Vanilla JS

Testing	Pytest

Deployment	Docker + AWS EC2

CI/CD	GitHub Actions

ğŸ“¦ Project Structure

doc-rag-assistant/
â”‚
â”œâ”€â”€ src/app/

â”‚ â”œâ”€â”€ api/ (API Routes)

â”‚ â”‚ â”œâ”€â”€ documents.py

â”‚ â”‚ â””â”€â”€ qa.py

â”‚ â”œâ”€â”€ services/ (Core Business Logic)

â”‚ â”‚ â”œâ”€â”€ ingestion.py

â”‚ â”‚ â”œâ”€â”€ vectorstore.py

â”‚ â”‚ â””â”€â”€ rag.py

â”‚ â”œâ”€â”€ frontend/ (Web UI)

â”‚ â”‚ â”œâ”€â”€ index.html

â”‚ â”‚ â”œâ”€â”€ app.js

â”‚ â”‚ â””â”€â”€ styles.css

â”‚ â””â”€â”€ main.py (FastAPI root)

â”‚
â”œâ”€â”€ tests/ (Pytest)

â”œâ”€â”€ Dockerfile

â”œâ”€â”€ docker-compose.yml

â”œâ”€â”€ requirements.txt

â”œâ”€â”€ README.md

â””â”€â”€ .github/workflows/ci.yml

ğŸ› ï¸ Local Setup

1ï¸âƒ£ Create & activate virtual environment

python -m venv .venv

source .venv/bin/activate (Linux/Mac)

.venv\Scripts\activate (Windows)

2ï¸âƒ£ Install dependencies

pip install -r requirements.txt

3ï¸âƒ£ Start FastAPI

uvicorn app.main:app --reload --app-dir src

Access the app:

Frontend â†’ http://127.0.0.1:8000/frontend/

Docs â†’ http://127.0.0.1:8000/docs

ğŸ³ Docker Deployment

Build and run locally:

docker compose up --build

â˜ï¸ AWS EC2 Deployment (Dockerized)

On EC2 instance:

sudo apt update && sudo apt install docker.io -y

sudo systemctl enable docker

docker compose up --build -d

Then access:

http://<EC2-PUBLIC-IP>:8000/frontend/

ğŸ“Š MLflow Logging

Logged automatically:

âœ” Answer Latency

âœ” Number of Sources

âœ” Answer Quality

âœ” Query Length

Runs stored in: mlruns folder

ğŸ§ª Testing

Run unit tests:

pytest -v

ğŸ¤– CI/CD Workflow (GitHub Actions)

Included workflow (.github/workflows/ci.yml) runs:

âœ” Install dependencies

âœ” Run pytest

âœ” Build Docker image

Triggers: push, pull_request

ğŸ“¡ Future Enhancements

ğŸ”¹ Kubernetes deployment (EKS/Minikube)

ğŸ”¹ Prometheus Metrics & Grafana Dashboard

ğŸ”¹ Multi-tenant document support

ğŸ”¹ Authentication / JWT
